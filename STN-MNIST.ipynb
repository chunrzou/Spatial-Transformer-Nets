{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUxumAurQzME"
   },
   "source": [
    "The code of Spatial Transformer Networks experimenting on MNIST is based on the code in github: https://github.com/zsdonghao/Spatial-Transformer-Nets.git <br>\n",
    "The code here is part of the progress on learning-based registration project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfHpUbCibcfY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import Functions \n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import imp\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Models' from 'D:\\\\GoogleDrive\\\\Github\\\\Spatial-Transformer-Nets\\\\Models.py'>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(Functions)\n",
    "imp.reload(Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13371894571792659663\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.backend' has no attribute 'tensorflow_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-eb8f9dc10472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_dw_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accessing local variables before they are created.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[0;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.backend' has no attribute 'tensorflow_backend'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 0} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the original datasets\n",
    "## --------------------------------\n",
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape((-1,28,28,1))\n",
    "X_test = X_test.reshape((-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:19, 62.15it/s]\n",
      "200it [00:03, 59.28it/s]\n"
     ]
    }
   ],
   "source": [
    "## Enlarge datasets with distortion\n",
    "## --------------------------------\n",
    "orig_dims = (28,28)\n",
    "input_dims = (28,28,1)\n",
    "\n",
    "X_train_input = Functions.pad_distort_ims_fn(X_train,output_size=input_dims)\n",
    "X_test_input = Functions.pad_distort_ims_fn(X_test,output_size=input_dims)\n",
    "\n",
    "X_train_input = np.expand_dims(X_train_input,axis=-1) \n",
    "X_test_input = np.expand_dims(X_test_input,axis=-1) \n",
    "\n",
    "X_train_input = X_train_input /255.0\n",
    "X_test_input = X_test_input /255.0\n",
    "\n",
    "y_train_vec = to_categorical(y_train)\n",
    "y_test_vec = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "index = np.random.randint(0,10000)\n",
    "\n",
    "plt.imshow(X_test_input[index].reshape((input_dims[0],input_dims[1])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_nin = tf.keras.layers.Input(shape=input_dims,name='input_layer')\n",
    "model_block = Models.affine_transformer_mnist_model(input_dims,output_dims)\n",
    "\n",
    "model_output = model_block(layer_nin)\n",
    "model = tf.keras.models.Model(inputs=layer_nin,outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.01,momentum=0.9)\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=sgd,loss=\"categorical_crossentropy\",metrics=['categorical_accuracy'])\n",
    "#model.compile(optimizer=adam,loss=\"categorical_crossentropy\",metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12416/60000 [=====>........................] - ETA: 2:34 - loss: 2.3042 - categorical_accuracy: 0.09 - ETA: 1:05 - loss: 2.3052 - categorical_accuracy: 0.09 - ETA: 46s - loss: 2.3067 - categorical_accuracy: 0.0938 - ETA: 38s - loss: 2.3075 - categorical_accuracy: 0.099 - ETA: 37s - loss: 2.3091 - categorical_accuracy: 0.096 - ETA: 33s - loss: 2.3095 - categorical_accuracy: 0.097 - ETA: 31s - loss: 2.3068 - categorical_accuracy: 0.099 - ETA: 29s - loss: 2.3052 - categorical_accuracy: 0.099 - ETA: 27s - loss: 2.3055 - categorical_accuracy: 0.101 - ETA: 27s - loss: 2.3058 - categorical_accuracy: 0.100 - ETA: 26s - loss: 2.3048 - categorical_accuracy: 0.101 - ETA: 26s - loss: 2.3043 - categorical_accuracy: 0.101 - ETA: 25s - loss: 2.3044 - categorical_accuracy: 0.101 - ETA: 25s - loss: 2.3043 - categorical_accuracy: 0.103 - ETA: 24s - loss: 2.3039 - categorical_accuracy: 0.102 - ETA: 24s - loss: 2.3040 - categorical_accuracy: 0.102 - ETA: 23s - loss: 2.3038 - categorical_accuracy: 0.103 - ETA: 23s - loss: 2.3036 - categorical_accuracy: 0.102 - ETA: 23s - loss: 2.3037 - categorical_accuracy: 0.101 - ETA: 22s - loss: 2.3033 - categorical_accuracy: 0.101 - ETA: 22s - loss: 2.3032 - categorical_accuracy: 0.101 - ETA: 22s - loss: 2.3033 - categorical_accuracy: 0.100 - ETA: 22s - loss: 2.3031 - categorical_accuracy: 0.099 - ETA: 21s - loss: 2.3026 - categorical_accuracy: 0.101 - ETA: 21s - loss: 2.3027 - categorical_accuracy: 0.101 - ETA: 21s - loss: 2.3025 - categorical_accuracy: 0.103 - ETA: 21s - loss: 2.3021 - categorical_accuracy: 0.105 - ETA: 21s - loss: 2.3020 - categorical_accuracy: 0.106 - ETA: 20s - loss: 2.3016 - categorical_accuracy: 0.106 - ETA: 20s - loss: 2.3015 - categorical_accuracy: 0.106 - ETA: 20s - loss: 2.3012 - categorical_accuracy: 0.107 - ETA: 20s - loss: 2.3012 - categorical_accuracy: 0.107 - ETA: 20s - loss: 2.3005 - categorical_accuracy: 0.110 - ETA: 20s - loss: 2.3006 - categorical_accuracy: 0.109 - ETA: 20s - loss: 2.3006 - categorical_accuracy: 0.109 - ETA: 20s - loss: 2.3003 - categorical_accuracy: 0.110 - ETA: 19s - loss: 2.3003 - categorical_accuracy: 0.109 - ETA: 19s - loss: 2.3003 - categorical_accuracy: 0.109 - ETA: 19s - loss: 2.3002 - categorical_accuracy: 0.110 - ETA: 19s - loss: 2.2998 - categorical_accuracy: 0.111 - ETA: 19s - loss: 2.2997 - categorical_accuracy: 0.111 - ETA: 19s - loss: 2.2996 - categorical_accuracy: 0.112 - ETA: 18s - loss: 2.2996 - categorical_accuracy: 0.113 - ETA: 18s - loss: 2.2995 - categorical_accuracy: 0.114 - ETA: 18s - loss: 2.2993 - categorical_accuracy: 0.115 - ETA: 18s - loss: 2.2992 - categorical_accuracy: 0.116 - ETA: 18s - loss: 2.2991 - categorical_accuracy: 0.118 - ETA: 18s - loss: 2.2990 - categorical_accuracy: 0.118 - ETA: 18s - loss: 2.2985 - categorical_accuracy: 0.119 - ETA: 17s - loss: 2.2984 - categorical_accuracy: 0.121 - ETA: 17s - loss: 2.2982 - categorical_accuracy: 0.121 - ETA: 17s - loss: 2.2979 - categorical_accuracy: 0.123 - ETA: 17s - loss: 2.2977 - categorical_accuracy: 0.125 - ETA: 17s - loss: 2.2974 - categorical_accuracy: 0.127 - ETA: 17s - loss: 2.2972 - categorical_accuracy: 0.1281"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-614e35394cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs=100\n",
    "model_v1.fit(X_train_input,y_train_vec,validation_data=(X_test_input,y_test_vec),epochs=epochs,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_net = tf.keras.models.Model(inputs=model.input,outputs=t_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "index = np.random.randint(0,10000)\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.imshow(X_test_input[index].reshape((28,28)))\n",
    "recon = recon_net.predict(X_test_input[index].reshape(1,28,28,1))\n",
    "ax2.imshow(recon.reshape((28,28)))\n",
    "plt.show()\n",
    "#print(np.max(X_test_28[index]),np.min(X_test_40[index]))\n",
    "#print(np.max(recon),np.min(recon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_40[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Images\n",
    "\n",
    "# load 4 cat images\n",
    "img1 = X_train_40[1]\n",
    "img2 = X_train_40[2]\n",
    "img3 = X_train_40[3]\n",
    "img4 = X_train_40[4]\n",
    "\n",
    "input_img = np.stack([img1, img2, img3, img4], axis=0)\n",
    "B, H, W, C = input_img.shape\n",
    "print(\"Input Img Shape: {}\".format(input_img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identity Transform\n",
    "\n",
    "theta = np.array([[100., 0, 0], [0, 100., 0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, H, W, C])\n",
    "\n",
    "with tf.variable_scope('spatial_transformer'):\n",
    "    theta = theta.astype('float32')\n",
    "    theta = theta.flatten()\n",
    "\n",
    "    # define loc net weight and bias\n",
    "    loc_in = H*W*C\n",
    "    loc_out = 6\n",
    "    W_loc = tf.Variable(tf.zeros([loc_in, loc_out]), name='W_loc')\n",
    "    b_loc = tf.Variable(initial_value=theta, name='b_loc')\n",
    "    \n",
    "    # tie everything together\n",
    "    fc_loc = tf.tanh(tf.matmul(tf.zeros([B, loc_in]), W_loc) + b_loc)\n",
    "    h_trans = Functions.spatial_transformer_network(x, fc_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y = sess.run(h_trans, feed_dict={x: input_img})\n",
    "print(\"y: {}\".format(y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2,4)\n",
    "ax1.imshow(input_img[0].reshape((40,40)))\n",
    "ax2.imshow(y[0].reshape((40,40)))\n",
    "ax3.imshow(input_img[1].reshape((40,40)))\n",
    "ax4.imshow(y[1].reshape((40,40)))\n",
    "ax5.imshow(input_img[2].reshape((40,40)))\n",
    "ax6.imshow(y[2].reshape((40,40)))\n",
    "ax7.imshow(input_img[3].reshape((40,40)))\n",
    "ax8.imshow(y[3].reshape((40,40)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize affine transform tensor `theta`\n",
    "import math\n",
    "degree = 45\n",
    "theta = np.array([\n",
    "    [np.cos(math.radians(degree)), -np.sin(math.radians(degree)), 0], \n",
    "    [np.sin(math.radians(degree)), np.cos(math.radians(degree)), 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, H, W, C])\n",
    "\n",
    "with tf.variable_scope('spatial_transformer'):\n",
    "    theta = theta.astype('float32')\n",
    "    theta = theta.flatten()\n",
    "\n",
    "    # define loc net weight and bias\n",
    "    loc_in = H*W*C\n",
    "    loc_out = 6\n",
    "    W_loc = tf.Variable(tf.zeros([loc_in, loc_out]), name='W_loc')\n",
    "    b_loc = tf.Variable(initial_value=theta, name='b_loc')\n",
    "    \n",
    "    # tie everything together\n",
    "    fc_loc = tf.matmul(tf.zeros([B, loc_in]), W_loc) + b_loc\n",
    "    h_trans = Functions.spatial_transformer_network(x, fc_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y = sess.run(h_trans, feed_dict={x: input_img})\n",
    "print(\"y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2,4)\n",
    "ax1.imshow(input_img[0].reshape((40,40)))\n",
    "ax2.imshow(y[0].reshape((40,40)))\n",
    "ax3.imshow(input_img[1].reshape((40,40)))\n",
    "ax4.imshow(y[1].reshape((40,40)))\n",
    "ax5.imshow(input_img[2].reshape((40,40)))\n",
    "ax6.imshow(y[2].reshape((40,40)))\n",
    "ax7.imshow(input_img[3].reshape((40,40)))\n",
    "ax8.imshow(y[3].reshape((40,40)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize affine transform tensor `theta`\n",
    "import math\n",
    "degree = 45\n",
    "theta = np.array([\n",
    "    [-1*np.cos(math.radians(degree)), -1*-np.sin(math.radians(degree)), 0], \n",
    "    [-1*np.sin(math.radians(degree)), -1*np.cos(math.radians(degree)), 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, H, W, C])\n",
    "\n",
    "with tf.variable_scope('spatial_transformer'):\n",
    "    theta = theta.astype('float32')\n",
    "    theta = theta.flatten()\n",
    "\n",
    "    # define loc net weight and bias\n",
    "    loc_in = H*W*C\n",
    "    loc_out = 6\n",
    "    W_loc = tf.Variable(tf.zeros([loc_in, loc_out]), name='W_loc')\n",
    "    b_loc = tf.Variable(initial_value=theta, name='b_loc')\n",
    "    \n",
    "    # tie everything together\n",
    "    fc_loc = tf.matmul(tf.zeros([B, loc_in]), W_loc) + b_loc\n",
    "    h_trans = Functions.spatial_transformer_network(x, fc_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y = sess.run(h_trans, feed_dict={x: input_img})\n",
    "print(\"y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2,4)\n",
    "ax1.imshow(input_img[0].reshape((40,40)))\n",
    "ax2.imshow(y[0].reshape((40,40)))\n",
    "ax3.imshow(input_img[1].reshape((40,40)))\n",
    "ax4.imshow(y[1].reshape((40,40)))\n",
    "ax5.imshow(input_img[2].reshape((40,40)))\n",
    "ax6.imshow(y[2].reshape((40,40)))\n",
    "ax7.imshow(input_img[3].reshape((40,40)))\n",
    "ax8.imshow(y[3].reshape((40,40)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize affine transform tensor `theta`\n",
    "import math\n",
    "degree = 45\n",
    "theta = np.array([\n",
    "    [3*np.cos(math.radians(degree)), 3*-np.sin(math.radians(degree)), 0], \n",
    "    [3*np.sin(math.radians(degree)), 3*np.cos(math.radians(degree)), 0]\n",
    "])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, H, W, C])\n",
    "\n",
    "with tf.variable_scope('spatial_transformer'):\n",
    "    theta = theta.astype('float32')\n",
    "    theta = theta.flatten()\n",
    "\n",
    "    # define loc net weight and bias\n",
    "    loc_in = H*W*C\n",
    "    loc_out = 6\n",
    "    W_loc = tf.Variable(tf.zeros([loc_in, loc_out]), name='W_loc')\n",
    "    b_loc = tf.Variable(initial_value=theta, name='b_loc')\n",
    "    \n",
    "    # tie everything together\n",
    "    fc_loc = tf.matmul(tf.zeros([B, loc_in]), W_loc) + b_loc\n",
    "    h_trans = Functions.spatial_transformer_network(x, fc_loc)\n",
    "    \n",
    "# run session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y = sess.run(h_trans, feed_dict={x: input_img})\n",
    "print(\"y: {}\".format(y.shape))\n",
    "\n",
    "fig,((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2,4)\n",
    "ax1.imshow(input_img[0].reshape((40,40)))\n",
    "ax2.imshow(y[0].reshape((40,40)))\n",
    "ax3.imshow(input_img[1].reshape((40,40)))\n",
    "ax4.imshow(y[1].reshape((40,40)))\n",
    "ax5.imshow(input_img[2].reshape((40,40)))\n",
    "ax6.imshow(y[2].reshape((40,40)))\n",
    "ax7.imshow(input_img[3].reshape((40,40)))\n",
    "ax8.imshow(y[3].reshape((40,40)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STN-MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
